import os
import io
import wave
import asyncio
import tempfile
import numpy as np
import av
from datetime import datetime
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import edge_tts

from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder



st.write("ğŸ” DEBUG: secretsã‚­ãƒ¼ä¸€è¦§ â†’", list(st.secrets.keys()))
st.write("ğŸ” OPENAIã‚­ãƒ¼(secrets):", st.secrets.get("OPENAI_API_KEY", "None"))
st.write("ğŸ” OPENAIã‚­ãƒ¼(env):", os.getenv("OPENAI_API_KEY", "None"))




# --- âœ… Pydanticã‚¨ãƒ©ãƒ¼å›é¿ ---
ChatOpenAI.model_rebuild()

# --- ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ---
load_dotenv()

# âœ… Secrets ã¨ .env ã®ä¸¡å¯¾å¿œ
OPENAI_API_KEY = None
if "OPENAI_API_KEY" in st.secrets:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
elif os.getenv("OPENAI_API_KEY"):
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloud ã® Secrets ã¾ãŸã¯ .env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- OpenAI åˆæœŸåŒ– ---
client = OpenAI(api_key=OPENAI_API_KEY)

# --- LangChain Memory åˆæœŸåŒ– ---
if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = ConversationBufferMemory(return_messages=True)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.6, api_key=OPENAI_API_KEY)



# --- AIå¿œç­”ç”Ÿæˆ ---
def get_ai_reply(user_text: str) -> str:
    memory = st.session_state.conversation_memory

    system_prompt = """
ã‚ãªãŸã¯å„ªã—ã„è‹±ä¼šè©±è¬›å¸«ã§ã™ã€‚
å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š
1è¡Œç›®ï¼šè‹±èªã§è‡ªç„¶ãªè¿”ç­”ï¼ˆCEFR B1-B2ãƒ¬ãƒ™ãƒ«ã€ä¼šè©±ã‚’ç¶šã‘ã‚‹è³ªå•ã‚‚å«ã‚€ï¼‰
2è¡Œç›®ä»¥é™ï¼šã€Œæ—¥æœ¬èªè¨³ï¼šã€ã§å§‹ã‚ã¦ç¿»è¨³
æœ€å¾Œã«ã€Œå­¦ç¿’ãƒã‚¤ãƒ³ãƒˆï¼šã€ã§1ã€œ3å€‹ç°¡æ½”ã«èª¬æ˜
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    chain = prompt | llm
    response = chain.invoke({
        "input": user_text,
        "history": memory.load_memory_variables({}).get("history", []),
    })

    reply = response.content
    memory.chat_memory.add_user_message(user_text)
    memory.chat_memory.add_ai_message(reply)
    return reply


# --- Whisperæ–‡å­—èµ·ã“ã— ---
def transcribe_audio(wav_bytes: bytes) -> str:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(wav_bytes)
        tmp.flush()
        tmp_path = tmp.name

    with open(tmp_path, "rb") as f:
        result = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            language="en",
        )
    return result.text.strip()


# --- Edge-TTS éŸ³å£°åˆæˆ ---
async def _edge_tts_to_file(text: str, voice: str, out_path: str):
    tts = edge_tts.Communicate(text, voice=voice)
    await tts.save(out_path)

def synthesize_speech(text: str, voice="en-US-JennyNeural") -> str:
    if not text.strip():
        return ""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        out_path = tmp.name
    asyncio.run(_edge_tts_to_file(text, voice, out_path))
    return out_path


# --- WebRTC éŒ²éŸ³å‡¦ç† ---
class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.frames = []

    def recv_audio(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.frames.append(frame)
        return frame


def frames_to_wav_bytes(frames) -> bytes:
    if not frames:
        raise ValueError("éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ã™ã€‚")

    sample_rate = frames[0].sample_rate or 48000
    pcm_list = []
    for f in frames:
        a = f.to_ndarray()
        if a.ndim == 2:
            a = a[0]
        pcm_list.append(a)
    pcm = np.concatenate(pcm_list).astype(np.int16)

    buf = io.BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(pcm.tobytes())
    return buf.getvalue()


def extract_english_part(reply: str) -> str:
    if "æ—¥æœ¬èªè¨³" in reply:
        return reply.split("æ—¥æœ¬èªè¨³")[0].strip()
    return reply.split("\n")[0].strip()


# --- ãƒ¡ã‚¤ãƒ³UI ---
def show_english_conversation():
    st.title("ğŸ§ è‹±ä¼šè©±ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆã‚¹ãƒãƒ›å¯¾å¿œãƒ»WebRTCç‰ˆï¼‰")
    st.caption("ğŸ™ï¸ Start â†’ Stop â†’ ã“ã®éŒ²éŸ³ã§AIã«é€ä¿¡")

    col1, col2 = st.columns(2)
    with col1:
        voice = st.selectbox(
            "AIéŸ³å£°ã‚¿ã‚¤ãƒ—",
            ["en-US-JennyNeural", "en-US-GuyNeural", "en-GB-RyanNeural"],
            index=0,
        )
    with col2:
        st.caption("WebRTCéŒ²éŸ³ â†’ Whisperèªè­˜ â†’ ChatGPTå¿œç­” â†’ Edge-TTSå†ç”Ÿ")

    st.markdown("---")

    ctx = webrtc_streamer(
        key="mobile-english-conversation",
        mode=WebRtcMode.SENDONLY,
        audio_receiver_size=1024,
        media_stream_constraints={"audio": True, "video": False},
        async_processing=True,
        audio_processor_factory=AudioProcessor,
    )

    if not ctx.state.playing:
        st.info("ğŸ™ï¸ Startãƒœã‚¿ãƒ³ã§éŒ²éŸ³é–‹å§‹ â†’ Stopãƒœã‚¿ãƒ³ã§çµ‚äº†ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.success("ğŸ”´ éŒ²éŸ³ä¸­ã§ã™ã€‚Stopã‚’æŠ¼ã—ã¦çµ‚äº†ã€‚")

    if ctx.audio_processor and st.button("ğŸ¯ ã“ã®éŒ²éŸ³ã§AIã«é€ä¿¡"):
        frames = ctx.audio_processor.frames
        ctx.audio_processor.frames = []

        if not frames:
            st.warning("âš ï¸ éŸ³å£°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚éŒ²éŸ³ãŒçŸ­ã™ããŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return

        try:
            wav_bytes = frames_to_wav_bytes(frames)
        except Exception as e:
            st.error(f"éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return

        st.audio(wav_bytes, format="audio/wav")

        with st.spinner("ğŸ§ Whisperã§éŸ³å£°ã‚’è§£æä¸­..."):
            try:
                user_text = transcribe_audio(wav_bytes)
            except Exception as e:
                st.error(f"éŸ³å£°èªè­˜å¤±æ•—: {e}")
                return

        if not user_text:
            st.warning("âš ï¸ éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©±ã—ã¦ãã ã•ã„ã€‚")
            return

        st.markdown(f"**ğŸ—£ ã‚ãªãŸ:** {user_text}")

        with st.spinner("ğŸ¤– ChatGPTãŒå¿œç­”ä¸­..."):
            reply = get_ai_reply(user_text)
        st.markdown("**ğŸ¤– AIã®è¿”ç­”:**")
        st.markdown(reply)

        english_part = extract_english_part(reply)
        if english_part:
            with st.spinner("ğŸ”Š éŸ³å£°ç”Ÿæˆä¸­..."):
                audio_path = synthesize_speech(english_part, voice=voice)
                if audio_path:
                    st.audio(audio_path, format="audio/mp3")

    st.markdown("---")
    st.subheader("ğŸ’¬ ä¼šè©±å±¥æ­´ï¼ˆä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
    history = st.session_state.conversation_memory.load_memory_variables({}).get("history", [])
    if history:
        for m in history:
            role = "ğŸ‘¤ You" if m.type == "human" else "ğŸ¤– AI"
            st.markdown(f"**{role}:** {m.content}")
    else:
        st.caption("ã¾ã ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Start â†’ Stop â†’ é€ä¿¡ã§ä¼šè©±ã‚’å§‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")
